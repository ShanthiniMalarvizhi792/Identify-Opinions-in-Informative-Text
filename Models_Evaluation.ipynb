{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c4bf6d-7ff8-46b2-9b82-911520c15eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c2113",
   "metadata": {},
   "source": [
    "**INFORMATION MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23aa3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\I747613\\AppData\\Local\\Temp\\ipykernel_33816\\4225305834.py\", line 8, in <module>\n",
      "    model_opinion = AutoModelForCausalLM.from_pretrained(path_to_files).to('cpu')  # Load model on CPU\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 562, in from_pretrained\n",
      "    model_class = _get_model_class(config, cls._model_mapping)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 383, in _get_model_class\n",
      "    supported_models = model_mapping[type(config)]\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 734, in __getitem__\n",
      "    return self._load_attr_from_module(model_type, model_name)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 748, in _load_attr_from_module\n",
      "    return getattribute_from_module(self._modules[module_name], attr)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 692, in getattribute_from_module\n",
      "    if hasattr(module, attr):\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1525, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1535, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py\", line 34, in <module>\n",
      "    from ...modeling_utils import PreTrainedModel\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\modeling_utils.py\", line 45, in <module>\n",
      "    from .generation import GenerationConfig, GenerationMixin\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1525, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1535, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\transformers\\generation\\utils.py\", line 97, in <module>\n",
      "    from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\accelerate\\__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\accelerate\\accelerator.py\", line 35, in <module>\n",
      "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\accelerate\\checkpointing.py\", line 24, in <module>\n",
      "    from .utils import (\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\accelerate\\utils\\__init__.py\", line 183, in <module>\n",
      "    from .fsdp_utils import load_fsdp_model, load_fsdp_optimizer, merge_fsdp_weights, save_fsdp_model, save_fsdp_optimizer\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\accelerate\\utils\\fsdp_utils.py\", line 29, in <module>\n",
      "    import torch.distributed.checkpoint as dist_cp\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\distributed\\checkpoint\\__init__.py\", line 2, in <module>\n",
      "    from .default_planner import DefaultLoadPlanner, DefaultSavePlanner\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\distributed\\checkpoint\\default_planner.py\", line 13, in <module>\n",
      "    from torch.distributed._tensor import DTensor\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\distributed\\_tensor\\__init__.py\", line 6, in <module>\n",
      "    import torch.distributed._tensor.ops\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\distributed\\_tensor\\ops\\__init__.py\", line 2, in <module>\n",
      "    from .embedding_ops import *  # noqa: F403\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\distributed\\_tensor\\ops\\embedding_ops.py\", line 8, in <module>\n",
      "    import torch.distributed._functional_collectives as funcol\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\distributed\\_functional_collectives.py\", line 12, in <module>\n",
      "    from . import _functional_collectives_impl as fun_col_impl\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\distributed\\_functional_collectives_impl.py\", line 36, in <module>\n",
      "    from torch._dynamo import assume_constant_result\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\__init__.py\", line 64, in <module>\n",
      "    torch.manual_seed = disable(torch.manual_seed)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\decorators.py\", line 50, in disable\n",
      "    return DisableContext()(fn)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 410, in __call__\n",
      "    (filename is None or trace_rules.check(fn))\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3378, in check\n",
      "    return check_verbose(obj, is_inlined_call).skipped\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3361, in check_verbose\n",
      "    rule = torch._dynamo.trace_rules.lookup_inner(\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3442, in lookup_inner\n",
      "    rule = get_torch_obj_rule_map().get(obj, None)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2782, in get_torch_obj_rule_map\n",
      "    obj = load_object(k)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2811, in load_object\n",
      "    val = _load_obj_from_str(x[0])\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2795, in _load_obj_from_str\n",
      "    return getattr(importlib.import_module(module), obj_name)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py\", line 417, in <module>\n",
      "    values=torch.randn(3, 3, device=\"meta\"),\n",
      "c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py:417: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  values=torch.randn(3, 3, device=\"meta\"),\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012f32ca32ba44ffa28f0578a472be10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your files\n",
    "path_to_files = \"information-model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_opinion = AutoTokenizer.from_pretrained(path_to_files)\n",
    "\n",
    "# Load the model configuration and weights\n",
    "model_opinion = AutoModelForCausalLM.from_pretrained(path_to_files).to('cpu')  # Load model on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cee0fa",
   "metadata": {},
   "source": [
    "**OPINION MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e54fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\i747613\\appdata\\local\\miniconda3\\envs\\lab\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5d520afe8b4ba393c68b387376ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the path to your files\n",
    "path_to_files = \"opinion-model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_information = AutoTokenizer.from_pretrained(path_to_files)\n",
    "\n",
    "# Load the model configuration and weights\n",
    "model_information = AutoModelForCausalLM.from_pretrained(path_to_files).to('cpu')  # Load model on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0becb",
   "metadata": {},
   "source": [
    "**NORMALIZE THE PROBABILITIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7b2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_probabilities(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=-1).cpu()\n",
    "    return probabilities\n",
    "\n",
    "def aggregate_probabilities(probabilities):\n",
    "        return torch.max(probabilities, dim=1).values\n",
    "\n",
    "def classify_text(text_probs_opinion, text_probs_information):\n",
    "    avg_opinion_prob = torch.mean(text_probs_opinion).item()\n",
    "    avg_information_prob = torch.mean(text_probs_information).item()\n",
    "    \n",
    "    if avg_opinion_prob > avg_information_prob:\n",
    "        return \"opinion\"\n",
    "    elif avg_information_prob > avg_opinion_prob:\n",
    "        return \"information\"\n",
    "    else:\n",
    "        return \"uncertain\"\n",
    "\n",
    "def classify_tokens(opinion_probs, information_probs):\n",
    "    # Max probabilities for each token\n",
    "    opinion_max_probs = opinion_probs[0, :, :].max(dim=-1).values\n",
    "    information_max_probs = information_probs[0, :, :].max(dim=-1).values\n",
    "    \n",
    "    # Classification based on probabilities\n",
    "    classifications = ['opinion' if o > i else 'information' if i > o else 'neutral'\n",
    "                       for o, i in zip(opinion_max_probs, information_max_probs)]\n",
    "    return classifications\n",
    "\n",
    "def highlight_text(sentence, tokenizer, opinion_probs, information_probs):\n",
    "    # Tokenize the input sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    # Re-tokenize the sentence to match tokenizer encoding\n",
    "    token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    words = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    # Classify tokens\n",
    "    classifications = classify_tokens(opinion_probs, information_probs)\n",
    "\n",
    "    # Highlight the text\n",
    "    highlighted_sentence = []\n",
    "    for word, classification in zip(words, classifications):\n",
    "        if classification == 'opinion':\n",
    "            color = 'red'\n",
    "        elif classification == 'information':\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'yellow'  # Uncertain\n",
    "        \n",
    "        highlighted_sentence.append(f\"<span style='color: {color}; padding: 2px;'>{word}</span>\")\n",
    "\n",
    "    return \" \".join(highlighted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f863457-373c-483a-b5e9-826bd701c6b4",
   "metadata": {},
   "source": [
    "**KAGGLE NEWSPAPER DATASET TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2364384-05c4-4127-bd56-5ebe8c2ff1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Headlines from the kaggle newspaper dataset\n",
    "headlines = [\n",
    "    \"Supreme Court Rules Biden Properly Ended Trump's 'Remain In Mexico' Policy.\",\n",
    "    \"How Tim Walz has already changed the campaign.\",\n",
    "    \"With the closure of checkpoints, Israeli Arabs cannot come to Jenin and Tulkarm to shop, and West Bank Palestinians cannot leave to work in Israel.\",\n",
    "    \"Winning a gold medal means taking home a piece of the Eiffel Tower.\",\n",
    "    \"Ukraine: 9,000 Of Its Troops Killed Since Russia Began War.\" ,\n",
    "    \"195 House Republicans Voted Against Birth Control Protections.\",\n",
    "    \"Ex-DHS Aide Suggests She 'Went Very Public' Because She Didn't Trust Inspector General.\",\n",
    "    \"Beautiful And Sad At The Same Time: Ukrainian Cultural Festival Takes On A Deeper Meaning This Year.\",\n",
    "    \"In A Nod To JFK, Joe Biden Pushing 'Moonshot' To Fight Cancer.\",\n",
    "    \"Come On, Bernie! Why Democrats Left Child Tax Credit Out Of The Inflation Reduction Act\"\n",
    "]\n",
    "# Manual Labels for each Headlines\n",
    "labels = [\n",
    "    \"Information\",\n",
    "    \"Opinion\",\n",
    "    \"Information\",\n",
    "    \"Opinion\",\n",
    "    \"Information\",\n",
    "    \"Information\",\n",
    "    \"Opinion\",\n",
    "    \"Opinion\",\n",
    "    \"Information\",\n",
    "    \"Opinion\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "010ff484-0bb7-4315-8a98-6a455928d4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> Supreme Court Rules Biden Properly Ended Trump's 'Remain In Mexico' Policy.</p><p><span style='color: green; padding: 2px;'>Supreme</span> <span style='color: red; padding: 2px;'>▁Court</span> <span style='color: green; padding: 2px;'>▁Rules</span> <span style='color: red; padding: 2px;'>▁Biden</span> <span style='color: green; padding: 2px;'>▁Properly</span> <span style='color: green; padding: 2px;'>▁Ended</span> <span style='color: green; padding: 2px;'>▁Trump</span> <span style='color: green; padding: 2px;'>'</span> <span style='color: green; padding: 2px;'>s</span> <span style='color: green; padding: 2px;'>▁'</span> <span style='color: red; padding: 2px;'>Remain</span> <span style='color: red; padding: 2px;'>▁In</span> <span style='color: red; padding: 2px;'>▁Mexico</span> <span style='color: green; padding: 2px;'>'</span> <span style='color: red; padding: 2px;'>▁Policy</span> <span style='color: green; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> INFORMATION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> How Tim Walz has already changed the campaign.</p><p><span style='color: green; padding: 2px;'>How</span> <span style='color: red; padding: 2px;'>▁Tim</span> <span style='color: red; padding: 2px;'>▁Wal</span> <span style='color: green; padding: 2px;'>z</span> <span style='color: red; padding: 2px;'>▁has</span> <span style='color: red; padding: 2px;'>▁already</span> <span style='color: green; padding: 2px;'>▁changed</span> <span style='color: red; padding: 2px;'>▁the</span> <span style='color: green; padding: 2px;'>▁campaign</span> <span style='color: green; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> OPINION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> With the closure of checkpoints, Israeli Arabs cannot come to Jenin and Tulkarm to shop, and West Bank Palestinians cannot leave to work in Israel.</p><p><span style='color: green; padding: 2px;'>With</span> <span style='color: red; padding: 2px;'>▁the</span> <span style='color: green; padding: 2px;'>▁closure</span> <span style='color: red; padding: 2px;'>▁of</span> <span style='color: green; padding: 2px;'>▁checkpoints</span> <span style='color: green; padding: 2px;'>,</span> <span style='color: red; padding: 2px;'>▁Israeli</span> <span style='color: red; padding: 2px;'>▁Arabs</span> <span style='color: red; padding: 2px;'>▁cannot</span> <span style='color: green; padding: 2px;'>▁come</span> <span style='color: red; padding: 2px;'>▁to</span> <span style='color: red; padding: 2px;'>▁Jen</span> <span style='color: red; padding: 2px;'>in</span> <span style='color: green; padding: 2px;'>▁and</span> <span style='color: green; padding: 2px;'>▁T</span> <span style='color: green; padding: 2px;'>ulk</span> <span style='color: green; padding: 2px;'>arm</span> <span style='color: green; padding: 2px;'>▁to</span> <span style='color: green; padding: 2px;'>▁shop</span> <span style='color: green; padding: 2px;'>,</span> <span style='color: red; padding: 2px;'>▁and</span> <span style='color: red; padding: 2px;'>▁West</span> <span style='color: red; padding: 2px;'>▁Bank</span> <span style='color: red; padding: 2px;'>▁Palestinians</span> <span style='color: red; padding: 2px;'>▁cannot</span> <span style='color: red; padding: 2px;'>▁leave</span> <span style='color: red; padding: 2px;'>▁to</span> <span style='color: green; padding: 2px;'>▁work</span> <span style='color: red; padding: 2px;'>▁in</span> <span style='color: red; padding: 2px;'>▁Israel</span> <span style='color: green; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> INFORMATION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> Winning a gold medal means taking home a piece of the Eiffel Tower.</p><p><span style='color: green; padding: 2px;'>Winning</span> <span style='color: green; padding: 2px;'>▁a</span> <span style='color: red; padding: 2px;'>▁gold</span> <span style='color: red; padding: 2px;'>▁medal</span> <span style='color: red; padding: 2px;'>▁means</span> <span style='color: red; padding: 2px;'>▁taking</span> <span style='color: red; padding: 2px;'>▁home</span> <span style='color: green; padding: 2px;'>▁a</span> <span style='color: red; padding: 2px;'>▁piece</span> <span style='color: red; padding: 2px;'>▁of</span> <span style='color: red; padding: 2px;'>▁the</span> <span style='color: red; padding: 2px;'>▁Eiffel</span> <span style='color: red; padding: 2px;'>▁Tower</span> <span style='color: green; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> OPINION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> Ukraine: 9,000 Of Its Troops Killed Since Russia Began War.</p><p><span style='color: green; padding: 2px;'>Ukraine</span> <span style='color: red; padding: 2px;'>:</span> <span style='color: green; padding: 2px;'>▁</span> <span style='color: red; padding: 2px;'>9</span> <span style='color: red; padding: 2px;'>,</span> <span style='color: red; padding: 2px;'>0</span> <span style='color: red; padding: 2px;'>0</span> <span style='color: red; padding: 2px;'>0</span> <span style='color: green; padding: 2px;'>▁Of</span> <span style='color: green; padding: 2px;'>▁Its</span> <span style='color: red; padding: 2px;'>▁Troops</span> <span style='color: red; padding: 2px;'>▁Killed</span> <span style='color: red; padding: 2px;'>▁Since</span> <span style='color: red; padding: 2px;'>▁Russia</span> <span style='color: green; padding: 2px;'>▁Began</span> <span style='color: red; padding: 2px;'>▁War</span> <span style='color: red; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> INFORMATION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> 195 House Republicans Voted Against Birth Control Protections.</p><p><span style='color: green; padding: 2px;'>1</span> <span style='color: green; padding: 2px;'>9</span> <span style='color: green; padding: 2px;'>5</span> <span style='color: red; padding: 2px;'>▁House</span> <span style='color: red; padding: 2px;'>▁Republicans</span> <span style='color: green; padding: 2px;'>▁Voted</span> <span style='color: green; padding: 2px;'>▁Against</span> <span style='color: red; padding: 2px;'>▁Birth</span> <span style='color: red; padding: 2px;'>▁Control</span> <span style='color: green; padding: 2px;'>▁Prote</span> <span style='color: red; padding: 2px;'>ctions</span> <span style='color: green; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> INFORMATION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> Ex-DHS Aide Suggests She 'Went Very Public' Because She Didn't Trust Inspector General.</p><p><span style='color: green; padding: 2px;'>Ex</span> <span style='color: green; padding: 2px;'>-</span> <span style='color: red; padding: 2px;'>D</span> <span style='color: green; padding: 2px;'>HS</span> <span style='color: red; padding: 2px;'>▁Aide</span> <span style='color: red; padding: 2px;'>▁Sugges</span> <span style='color: red; padding: 2px;'>ts</span> <span style='color: red; padding: 2px;'>▁She</span> <span style='color: red; padding: 2px;'>▁'</span> <span style='color: red; padding: 2px;'>Went</span> <span style='color: red; padding: 2px;'>▁Very</span> <span style='color: green; padding: 2px;'>▁Public</span> <span style='color: green; padding: 2px;'>'</span> <span style='color: red; padding: 2px;'>▁Because</span> <span style='color: red; padding: 2px;'>▁She</span> <span style='color: red; padding: 2px;'>▁Didn</span> <span style='color: red; padding: 2px;'>'</span> <span style='color: green; padding: 2px;'>t</span> <span style='color: red; padding: 2px;'>▁Trust</span> <span style='color: red; padding: 2px;'>▁Inspector</span> <span style='color: red; padding: 2px;'>▁General</span> <span style='color: green; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> OPINION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> Beautiful And Sad At The Same Time: Ukrainian Cultural Festival Takes On A Deeper Meaning This Year.</p><p><span style='color: green; padding: 2px;'>Beautiful</span> <span style='color: red; padding: 2px;'>▁And</span> <span style='color: red; padding: 2px;'>▁Sad</span> <span style='color: red; padding: 2px;'>▁At</span> <span style='color: red; padding: 2px;'>▁The</span> <span style='color: red; padding: 2px;'>▁Same</span> <span style='color: green; padding: 2px;'>▁Time</span> <span style='color: green; padding: 2px;'>:</span> <span style='color: red; padding: 2px;'>▁Ukrainian</span> <span style='color: red; padding: 2px;'>▁Cultural</span> <span style='color: red; padding: 2px;'>▁Festival</span> <span style='color: red; padding: 2px;'>▁Takes</span> <span style='color: red; padding: 2px;'>▁On</span> <span style='color: red; padding: 2px;'>▁A</span> <span style='color: red; padding: 2px;'>▁Deeper</span> <span style='color: red; padding: 2px;'>▁Meaning</span> <span style='color: red; padding: 2px;'>▁This</span> <span style='color: red; padding: 2px;'>▁Year</span> <span style='color: green; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> INFORMATION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> In A Nod To JFK, Joe Biden Pushing 'Moonshot' To Fight Cancer.</p><p><span style='color: green; padding: 2px;'>In</span> <span style='color: green; padding: 2px;'>▁A</span> <span style='color: red; padding: 2px;'>▁Nod</span> <span style='color: red; padding: 2px;'>▁To</span> <span style='color: green; padding: 2px;'>▁JFK</span> <span style='color: green; padding: 2px;'>,</span> <span style='color: green; padding: 2px;'>▁Joe</span> <span style='color: red; padding: 2px;'>▁Biden</span> <span style='color: green; padding: 2px;'>▁Pushing</span> <span style='color: red; padding: 2px;'>▁'</span> <span style='color: green; padding: 2px;'>Moons</span> <span style='color: red; padding: 2px;'>hot</span> <span style='color: red; padding: 2px;'>'</span> <span style='color: red; padding: 2px;'>▁To</span> <span style='color: red; padding: 2px;'>▁Fight</span> <span style='color: red; padding: 2px;'>▁Cancer</span> <span style='color: green; padding: 2px;'>.</span></p><p><strong>Model Output: </strong> <em> INFORMATION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Headline:</strong> Come On, Bernie! Why Democrats Left Child Tax Credit Out Of The Inflation Reduction Act</p><p><span style='color: green; padding: 2px;'>Come</span> <span style='color: red; padding: 2px;'>▁On</span> <span style='color: red; padding: 2px;'>,</span> <span style='color: red; padding: 2px;'>▁Bernie</span> <span style='color: green; padding: 2px;'>!</span> <span style='color: green; padding: 2px;'>▁Why</span> <span style='color: green; padding: 2px;'>▁Democrats</span> <span style='color: red; padding: 2px;'>▁Left</span> <span style='color: green; padding: 2px;'>▁Child</span> <span style='color: red; padding: 2px;'>▁Tax</span> <span style='color: red; padding: 2px;'>▁Credit</span> <span style='color: red; padding: 2px;'>▁Out</span> <span style='color: red; padding: 2px;'>▁Of</span> <span style='color: red; padding: 2px;'>▁The</span> <span style='color: green; padding: 2px;'>▁Inflation</span> <span style='color: red; padding: 2px;'>▁Reduction</span> <span style='color: red; padding: 2px;'>▁Act</span></p><p><strong>Model Output: </strong> <em> OPINION </em></p><hr style='border: 1px solid #ccc;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate and classify each input, then highlight the text based on token probabilities\n",
    "for input_text in headlines:\n",
    "    opinion_probs = evaluate_probabilities(model_opinion, tokenizer_opinion, input_text)\n",
    "    information_probs = evaluate_probabilities(model_information, tokenizer_information, input_text)\n",
    "\n",
    "    text_probs_opinion = aggregate_probabilities(opinion_probs)\n",
    "    text_probs_information = aggregate_probabilities(information_probs)\n",
    "    \n",
    "    highlighted_sentence = highlight_text(input_text, tokenizer_opinion, opinion_probs, information_probs)\n",
    "\n",
    "    classification = classify_text(text_probs_opinion, text_probs_information).upper()\n",
    "\n",
    "    output = f\"<p><strong>Headline:</strong> {input_text}</p>\" \\\n",
    "             f\"<p>{highlighted_sentence}</p>\" \\\n",
    "             f\"<p><strong>Model Output: </strong> <em> {classification} </em></p>\" \\\n",
    "             f\"<hr style='border: 1px solid #ccc;' />\"\n",
    "    \n",
    "    display(HTML(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
